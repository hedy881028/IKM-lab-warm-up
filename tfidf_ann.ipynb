{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-04T09:47:50.625734Z","iopub.execute_input":"2022-09-04T09:47:50.626249Z","iopub.status.idle":"2022-09-04T09:47:50.645787Z","shell.execute_reply.started":"2022-09-04T09:47:50.626174Z","shell.execute_reply":"2022-09-04T09:47:50.644200Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"t = pd.read_csv('../input/agnews/train.csv')\ntrain = t['Title']\nlabel_train = t['Class Index']\nlabel_train = [i-1 for i in label_train]\n\nt = pd.read_csv('../input/agnews/test.csv')\ntest = t['Title']\nlabel_test = t['Class Index']\nlabel_test = [i-1 for i in label_test]\nlabel_test = np.array(label_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T09:47:50.648292Z","iopub.execute_input":"2022-09-04T09:47:50.648912Z","iopub.status.idle":"2022-09-04T09:47:51.116617Z","shell.execute_reply.started":"2022-09-04T09:47:50.648859Z","shell.execute_reply":"2022-09-04T09:47:51.115231Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntrans = TfidfVectorizer(lowercase=True, stop_words='english', max_features = 1000, sublinear_tf=True)\nscore_train = trans.fit_transform(train)\nscore_test = trans.transform(test)\n\nscore_train = score_train.toarray() # sparse matrix (csr matrix) to numpy array\nscore_train = np.reshape(score_train, (-1, 10, 100))\nprint(score_train.shape)\n    \nscore_test = score_test.toarray()\nscore_test = np.reshape(score_test, (-1, 10, 100))","metadata":{"execution":{"iopub.status.busy":"2022-09-04T09:47:51.119324Z","iopub.execute_input":"2022-09-04T09:47:51.120239Z","iopub.status.idle":"2022-09-04T09:47:54.234959Z","shell.execute_reply.started":"2022-09-04T09:47:51.120193Z","shell.execute_reply":"2022-09-04T09:47:54.233494Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# artificial neural networks\nimport torch\nimport torch.nn as nn\n\nclass ann_model(nn.Module):\n    def __init__(self):\n        super(ann_model, self).__init__()\n        self.dim = 100\n        self.feature_num = 75\n        self.max_len = 10\n        self.label_num = 4\n        self.kernel_size = [3, 4, 5, 6]\n        \n        self.convs = nn.ModuleList([\n                     nn.Sequential(nn.Conv1d(self.dim, self.feature_num, h),\n                                   nn.BatchNorm1d(self.feature_num),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(self.max_len - h + 1)) for h in self.kernel_size])\n        self.fc = nn.Linear(self.feature_num * len(self.kernel_size), self.label_num)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        out = [conv(x) for conv in self.convs]\n        out = torch.cat(out, dim = 1)\n        out = out.view(-1, out.size(1))\n        out = self.fc(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-09-04T09:47:54.242237Z","iopub.execute_input":"2022-09-04T09:47:54.245124Z","iopub.status.idle":"2022-09-04T09:47:54.887987Z","shell.execute_reply.started":"2022-09-04T09:47:54.245080Z","shell.execute_reply":"2022-09-04T09:47:54.886615Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# preparation before training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\n\nf_train, f_val, l_train, l_val = train_test_split(score_train, label_train, test_size=0.3)\n\nclass dataset(Dataset):\n    def __init__(self, f, l):\n        self.data = f\n        self.label = l\n\n    def __getitem__(self, index):\n        return self.data[index], self.label[index]\n\n    def __len__(self):\n        return len(self.data)\n\nds_train = dataset(torch.from_numpy(f_train), torch.tensor(l_train))\ndl_train = DataLoader(dataset = ds_train, batch_size = 32, shuffle = True) # get 32 features and labels each time\n\ndef get_accu(output, label):\n    predict = torch.max(output, 1).indices\n    accu = accuracy_score(label.tolist(), predict.tolist())\n    matrix = confusion_matrix(label.tolist(), predict.tolist())\n            \n    return accu, matrix","metadata":{"execution":{"iopub.status.busy":"2022-09-04T09:47:54.889871Z","iopub.execute_input":"2022-09-04T09:47:54.891670Z","iopub.status.idle":"2022-09-04T09:47:55.421646Z","shell.execute_reply.started":"2022-09-04T09:47:54.891594Z","shell.execute_reply":"2022-09-04T09:47:55.420179Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# train\n# from torch.optim import Adam\nfrom torch.optim import SGD\nfrom torch.autograd import Variable\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nm = ann_model().double()\noptimizer = SGD(m.parameters(), lr=0.001, momentum=0.9)\nloss_func = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n    m = m.cuda()\n    loss_func = loss_func.cuda()\n    \n# train the model\ndef train(epoch):\n    for e in range(epoch):\n        this_epoch_loss = 0\n        this_epoch_accu = 0\n        \n        for i, (data, label) in enumerate(dl_train): # i for batch index\n            data, label = Variable(data), Variable(label)\n            \n            if torch.cuda.is_available():\n                data = data.cuda()\n                label = label.cuda()\n\n            optimizer.zero_grad()\n            output = m(data.double()) # output size: batch size * label num\n            loss = loss_func(output, label.long())\n            # calculate gradient\n            loss.backward()\n            # update parameters\n            optimizer.step()\n            \n            accu, _= get_accu(output, label)\n            this_epoch_accu += accu\n            this_epoch_loss += loss\n            \n        if (e + 1) % 5 == 0:\n            print('Training epoch: {}/{},  loss: {:.4f}, accu: {:.4f}'.format(e + 1, 100, this_epoch_loss / (i + 1), this_epoch_accu / (i + 1)))\n    \n# let's get it\ntrain(100)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T09:47:55.423535Z","iopub.execute_input":"2022-09-04T09:47:55.424400Z","iopub.status.idle":"2022-09-04T10:18:54.727276Z","shell.execute_reply.started":"2022-09-04T09:47:55.424358Z","shell.execute_reply":"2022-09-04T10:18:54.725863Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# validate\nval_f = torch.from_numpy(f_val)\nval_l = torch.tensor(list(l_val))\n\nval_f, val_l = Variable(val_f), Variable(val_l)\n\nif torch.cuda.is_available():\n    val_f = val_f.cuda()\n    val_l = val_l.cuda()\n\noutput = m(val_f.double())\nloss = loss_func(output, val_l.long())\nval_accu, mat = get_accu(output, val_l)\nval_loss = loss\n\nprint('Validation: loss: {:.4f}, accu: {:.4f}'.format(val_loss, val_accu))\n\nplt.figure(figsize = (6, 6))\nplt.matshow(mat, fignum = 1, cmap = plt.cm.Blues, alpha = 0.7)\nfor i in range(mat.shape[0]):\n    for j in range(mat.shape[1]):\n        plt.text(x = j, y = i, s = mat[i,j], va='center', ha='center')\n\ntarget_name = ['1', '2', '3', '4']\nplt.xticks(np.arange(4), target_name) \nplt.yticks(np.arange(4), target_name)\n        \nplt.title('Validation')\nplt.xlabel('predicted value')\nplt.ylabel('true value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-04T10:18:54.729468Z","iopub.execute_input":"2022-09-04T10:18:54.730247Z","iopub.status.idle":"2022-09-04T10:18:55.208928Z","shell.execute_reply.started":"2022-09-04T10:18:54.730215Z","shell.execute_reply":"2022-09-04T10:18:55.207489Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# test\ntest_f = torch.from_numpy(score_test)\n\nif torch.cuda.is_available():\n    m = m.cuda()\n    test_f = test_f.cuda()\n\noutput = m(test_f.double())\ntest_accu, mat = get_accu(output, label_test)\n\nprint('Test: accu: {:.4f}'.format(test_accu))\n\nplt.figure(figsize = (6, 6))\nplt.matshow(mat, fignum = 1, cmap = plt.cm.Blues, alpha = 0.7)\nfor i in range(mat.shape[0]):\n    for j in range(mat.shape[1]):\n        plt.text(x = j, y = i, s = mat[i,j], va='center', ha='center')\n\ntarget_name = ['1', '2', '3', '4']\nplt.xticks(np.arange(4), target_name) \nplt.yticks(np.arange(4), target_name)\n        \nplt.title('Test')\nplt.xlabel('predicted value')\nplt.ylabel('true value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-04T10:18:55.210985Z","iopub.execute_input":"2022-09-04T10:18:55.211521Z","iopub.status.idle":"2022-09-04T10:18:55.577606Z","shell.execute_reply.started":"2022-09-04T10:18:55.211478Z","shell.execute_reply":"2022-09-04T10:18:55.576401Z"},"trusted":true},"execution_count":8,"outputs":[]}]}