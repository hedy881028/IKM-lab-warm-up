{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-04T13:13:59.794669Z","iopub.execute_input":"2022-09-04T13:13:59.795817Z","iopub.status.idle":"2022-09-04T13:13:59.816373Z","shell.execute_reply.started":"2022-09-04T13:13:59.795700Z","shell.execute_reply":"2022-09-04T13:13:59.813997Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"t = pd.read_csv('../input/agnews/train.csv')\ntrain = t['Title']\nlabel_train = t['Class Index']\nlabel_train = [i-1 for i in label_train]\n\nt = pd.read_csv('../input/agnews/test.csv')\ntest = t['Title']\nlabel_test = t['Class Index']\nlabel_test = [i-1 for i in label_test]\nlabel_test = np.array(label_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:13:59.819509Z","iopub.execute_input":"2022-09-04T13:13:59.820341Z","iopub.status.idle":"2022-09-04T13:14:00.258516Z","shell.execute_reply.started":"2022-09-04T13:13:59.820299Z","shell.execute_reply":"2022-09-04T13:14:00.257542Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\ncorpus_train = []\ncorpus_test = []\npunc = '''()-[]{};:'\"\\,<>/@#$%^&*_.~”'''\nstop = set(stopwords.words('english'))\n\nfor i in train:\n    tokens = nltk.word_tokenize(i)\n    tokens = [w for w in tokens if w not in punc]\n    tokens = [w for w in tokens if w not in stop]\n    corpus_train.append(tokens)\n\nfor i in test:\n    tokens = nltk.word_tokenize(i)\n    tokens = [w for w in tokens if w not in punc]\n    tokens = [w for w in tokens if w not in stop]\n    corpus_test.append(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:14:00.260198Z","iopub.execute_input":"2022-09-04T13:14:00.260576Z","iopub.status.idle":"2022-09-04T13:14:14.344200Z","shell.execute_reply.started":"2022-09-04T13:14:00.260521Z","shell.execute_reply":"2022-09-04T13:14:14.343208Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from gensim.models import word2vec\nfrom gensim.models.word2vec import Word2Vec\n\nmodel = Word2Vec.load('../input/word2vec-model/wiki-lemma-100D-phrase')\n\ntrain_max = 0\ntest_max = 0\nmax_len = 0\n\nfor i in corpus_train:\n    if len(i) > train_max:\n        train_max = len(i)\n        \nfor i in corpus_test:\n    if len(i) > test_max:\n        test_max = len(i)\n        \nif train_max > test_max:\n    max_len = train_max\nelse:\n    max_len = test_max\n    \nfeature = []\nt_feature = []\n\n# size of corpus_train matrix of each corpus: max_len*100\nfor i in range(len(corpus_train)):\n    vec = np.zeros((max_len, 100))\n    \n    for j in range(len(corpus_train[i])):\n        if corpus_train[i][j] in model.wv.key_to_index:\n            vec[j] = model.wv[corpus_train[i][j]]\n        else:\n            vec[j] = np.zeros(100)\n\n    feature.append(vec)\n    \nfor i in range(len(corpus_test)):\n    vec = np.zeros((max_len, 100))\n    \n    for j in range(len(corpus_test[i])):\n        if corpus_test[i][j] in model.wv.key_to_index:\n            vec[j] = model.wv[corpus_test[i][j]]\n        else:\n            vec[j] = np.zeros(100)\n\n    t_feature.append(vec)\n\nfeature = np.array(feature)\nt_feature = np.array(t_feature)\nprint(feature.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:14:14.347050Z","iopub.execute_input":"2022-09-04T13:14:14.347429Z","iopub.status.idle":"2022-09-04T13:14:27.401216Z","shell.execute_reply.started":"2022-09-04T13:14:14.347393Z","shell.execute_reply":"2022-09-04T13:14:27.399405Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# artificial neural networks\nimport torch\nimport torch.nn as nn\n\nclass ann_model(nn.Module):\n    def __init__(self):\n        super(ann_model, self).__init__()\n        self.w2v_dim = 100\n        self.feature_num = 50\n        self.max_len = max_len\n        self.label_num = 4\n        self.kernel_size = [3, 4]\n        \n        self.convs = nn.ModuleList([\n                     nn.Sequential(nn.Conv1d(self.w2v_dim, self.feature_num, h),\n                                   nn.BatchNorm1d(self.feature_num),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(self.max_len - h + 1)) for h in self.kernel_size])\n        self.fc = nn.Linear(self.feature_num * len(self.kernel_size), self.label_num)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        out = [conv(x) for conv in self.convs]\n        out = torch.cat(out, dim = 1)\n        out = out.view(-1, out.size(1))\n        out = self.fc(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:14:27.402628Z","iopub.execute_input":"2022-09-04T13:14:27.403012Z","iopub.status.idle":"2022-09-04T13:14:27.842418Z","shell.execute_reply.started":"2022-09-04T13:14:27.402974Z","shell.execute_reply":"2022-09-04T13:14:27.841455Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# preparation before training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\n\nf_train, f_val, l_train, l_val = train_test_split(feature, label_train, test_size=0.3)\n\nclass dataset(Dataset):\n    def __init__(self, f, l):\n        self.data = f\n        self.label = l\n\n    def __getitem__(self, index):\n        return self.data[index], self.label[index]\n\n    def __len__(self):\n        return len(self.data)\n\nds_train = dataset(torch.from_numpy(f_train), torch.tensor(l_train))\ndl_train = DataLoader(dataset = ds_train, batch_size = 32, shuffle = True) # get 32 features and labels each time\n\ndef get_accu(output, label):\n    predict = torch.max(output, 1).indices\n    accu = accuracy_score(label.tolist(), predict.tolist())\n    matrix = confusion_matrix(label.tolist(), predict.tolist())\n            \n    return accu, matrix","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:14:27.844139Z","iopub.execute_input":"2022-09-04T13:14:27.844693Z","iopub.status.idle":"2022-09-04T13:14:28.412967Z","shell.execute_reply.started":"2022-09-04T13:14:27.844655Z","shell.execute_reply":"2022-09-04T13:14:28.411973Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# train\nfrom torch.optim import Adam\nfrom torch.optim import SGD\nfrom torch.autograd import Variable\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nm = ann_model().double()\noptimizer = Adam(m.parameters())\n# optimizer = SGD(m.parameters(), lr=0.001, momentum=0.9)\nloss_func = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n    m = m.cuda()\n    loss_func = loss_func.cuda()\n    \n# train the model\ndef train(epoch):\n    for e in range(epoch):\n        this_epoch_loss = 0\n        this_epoch_accu = 0\n        \n        for i, (data, label) in enumerate(dl_train): # i for batch index\n            data, label = Variable(data), Variable(label)\n            \n            if torch.cuda.is_available():\n                data = data.cuda()\n                label = label.cuda()\n\n            optimizer.zero_grad()\n            output = m(data.double()) # output size: batch size * label num\n            loss = loss_func(output, label.long())\n            # calculate gradient\n            loss.backward()\n            # update parameters\n            optimizer.step()\n            \n            accu, _= get_accu(output, label)\n            this_epoch_accu += accu\n            this_epoch_loss += loss\n            \n        if (e + 1) % 2 == 0:\n            print('Training epoch: {}/{},  loss: {:.4f}, accu: {:.4f}'.format(e + 1, 20, this_epoch_loss / (i + 1), this_epoch_accu / (i + 1)))\n    \n# let's get it\ntrain(20)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:14:28.414353Z","iopub.execute_input":"2022-09-04T13:14:28.414743Z","iopub.status.idle":"2022-09-04T13:19:16.598696Z","shell.execute_reply.started":"2022-09-04T13:14:28.414707Z","shell.execute_reply":"2022-09-04T13:19:16.597625Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# validate\nval_f = torch.from_numpy(f_val)\nval_l = torch.tensor(list(l_val))\n\nval_f, val_l = Variable(val_f), Variable(val_l)\n\nif torch.cuda.is_available():\n    val_f = val_f.cuda()\n    val_l = val_l.cuda()\n\noutput = m(val_f.double())\nloss = loss_func(output, val_l.long())\nval_accu, mat = get_accu(output, val_l)\nval_loss = loss\n\nprint('Validation: loss: {:.4f}, accu: {:.4f}'.format(val_loss, val_accu))\n\nplt.figure(figsize = (6, 6))\nplt.matshow(mat, fignum = 1, cmap = plt.cm.Blues, alpha = 0.7)\nfor i in range(mat.shape[0]):\n    for j in range(mat.shape[1]):\n        plt.text(x = j, y = i, s = mat[i,j], va='center', ha='center')\n\ntarget_name = ['1', '2', '3', '4']\nplt.xticks(np.arange(4), target_name) \nplt.yticks(np.arange(4), target_name)\n        \nplt.title('Validation')\nplt.xlabel('predicted value')\nplt.ylabel('true value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:19:16.610797Z","iopub.execute_input":"2022-09-04T13:19:16.611114Z","iopub.status.idle":"2022-09-04T13:19:17.110640Z","shell.execute_reply.started":"2022-09-04T13:19:16.611086Z","shell.execute_reply":"2022-09-04T13:19:17.109376Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# test\ntest_f = torch.from_numpy(t_feature)\n\nif torch.cuda.is_available():\n    m = m.cuda()\n    test_f = test_f.cuda()\n\noutput = m(test_f.double())\ntest_accu, mat = get_accu(output, label_test)\n\nprint('Test: accu: {:.4f}'.format(test_accu))\n\nplt.figure(figsize = (6, 6))\nplt.matshow(mat, fignum = 1, cmap = plt.cm.Blues, alpha = 0.7)\nfor i in range(mat.shape[0]):\n    for j in range(mat.shape[1]):\n        plt.text(x = j, y = i, s = mat[i,j], va='center', ha='center')\n\ntarget_name = ['1', '2', '3', '4']\nplt.xticks(np.arange(4), target_name) \nplt.yticks(np.arange(4), target_name)\n        \nplt.title('Test')\nplt.xlabel('predicted value')\nplt.ylabel('true value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-04T13:19:17.112066Z","iopub.execute_input":"2022-09-04T13:19:17.113783Z","iopub.status.idle":"2022-09-04T13:19:17.579959Z","shell.execute_reply.started":"2022-09-04T13:19:17.113731Z","shell.execute_reply":"2022-09-04T13:19:17.578973Z"},"trusted":true},"execution_count":9,"outputs":[]}]}